{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AngelTroncoso/Agentes_Gratis/blob/main/Chat_con_historial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejercicio: Chat Memory"
      ],
      "metadata": {
        "id": "cRYnIfkfENJv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "if \"OPENAI_API_KEY\" not in os.environ:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")"
      ],
      "metadata": {
        "id": "pcefg10sEQJO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain-openai langchain"
      ],
      "metadata": {
        "id": "V71RAFugERXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xhmeuYBxEJWB"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "model = ChatOpenAI(model=\"gpt-4o\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
        "\n",
        "chat_history = []\n",
        "if not chat_history:\n",
        "  system_message = SystemMessage(content='Eres un asistente útil')\n",
        "  chat_history.append(system_message)"
      ],
      "metadata": {
        "id": "FZUvmgOKEPn3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = input('Haz una pregunta: ')\n",
        "chat_history.append(HumanMessage(content=query))\n",
        "\n",
        "response = model.invoke(chat_history).content\n",
        "chat_history.append(AIMessage(content=response))"
      ],
      "metadata": {
        "id": "LS2jQ_-wEUw2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for message in chat_history:\n",
        "  print(message)"
      ],
      "metadata": {
        "id": "RiWuQ9s2EWIk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_history"
      ],
      "metadata": {
        "id": "ORxI_FEUEXq8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Resumen  \n",
        "En esta clase, hemos explorado cómo LangChain maneja el historial de chat, permitiendo que el modelo de lenguaje mantenga un contexto continuo durante una conversación. Esta funcionalidad es clave para construir sistemas conversacionales inteligentes, ya que el historial del chat almacena las interacciones anteriores, lo que le permite al modelo generar respuestas más precisas, contextuales y personalizadas.\n",
        "\n",
        "1. Importancia del Historial de Chat\n",
        "El historial de chat actúa como una memoria que guarda cada mensaje intercambiado entre el usuario y el modelo. Esto permite que el modelo:\n",
        "\n",
        "Recuerde detalles importantes que se mencionaron en interacciones anteriores.\n",
        "Mantenga la coherencia en la conversación, ofreciendo respuestas más personalizadas y relevantes.\n",
        "Imite una conversación más natural, ya que puede hacer referencia a lo discutido previamente sin que el usuario repita información.\n",
        "2. Creación del Historial\n",
        "Para gestionar el historial de chat, primero creamos una lista vacía que irá almacenando cada mensaje generado durante la interacción. Esta lista no solo contiene el contenido de los mensajes, sino también el rol (usuario, sistema o modelo de IA) para identificar quién generó cada parte del diálogo.\n",
        "\n",
        "System Message: Es el mensaje inicial que establece el contexto o el comportamiento del modelo. Por ejemplo, puedes configurarlo para que el modelo sepa que debe actuar como un “asistente útil”.\n",
        "Human Message: Cada pregunta o comentario hecho por el usuario se almacena en el historial con este rol.\n",
        "AI Message: Las respuestas generadas por el modelo también se guardan en el historial, lo que asegura que el contexto completo se mantenga.\n",
        "3. Ciclo de Conversación\n",
        "En cada interacción, el flujo básico es el siguiente:\n",
        "\n",
        "El sistema comprueba si hay un mensaje inicial (System Message) en el historial. Si no lo hay, se añade uno para dar contexto al asistente.\n",
        "El usuario hace una pregunta, que se almacena como un Human Message.\n",
        "El modelo genera una respuesta basada en el historial, y esta se guarda como un AI Message.\n",
        "Todo el historial se actualiza y puede ser reutilizado en las próximas interacciones.\n",
        "Este proceso se repite continuamente, añadiendo cada nueva interacción al historial, lo que garantiza que el modelo mantenga el contexto de toda la conversación.\n",
        "\n",
        "4. Ejecución y Depuración\n",
        "Durante el ejercicio, vimos cómo se interactúa con el modelo en tiempo real y cómo el historial de la conversación se actualiza con cada nuevo mensaje. Se mostró cómo imprimir y revisar el historial completo, que contiene el contexto del sistema, las entradas del usuario y las respuestas del modelo. Este historial permite que el asistente recuerde y dé seguimiento a cada consulta de forma lógica y coherente.\n",
        "\n",
        "5. Uso de Chains para Personalización\n",
        "Además del almacenamiento básico de mensajes, LangChain ofrece la posibilidad de integrar chains (cadenas de componentes) para personalizar las interacciones y las respuestas. Por ejemplo, puedes añadir cadenas que incluyan funciones adicionales, como herramientas de búsqueda, traducción automática o parsers que formateen la salida del modelo. Estas cadenas permiten que las respuestas sean aún más adaptadas a las necesidades del usuario y el contexto de la conversación.\n",
        "\n",
        "El manejo del historial de chat en LangChain es esencial para construir sistemas conversacionales avanzados que pueden recordar y referenciar interacciones anteriores. Esto no solo mejora la calidad y relevancia de las respuestas, sino que también proporciona una experiencia más fluida y natural para el usuario.\n",
        "\n",
        "Este tipo de flujo conversacional es fundamental en aplicaciones como:\n",
        "\n",
        "Asistentes virtuales: Que deben recordar preferencias del usuario.\n",
        "Chatbots de atención al cliente: Que necesitan mantener el contexto de problemas previos para ofrecer soluciones más eficientes.\n",
        "Sistemas educativos: Donde el asistente puede recordar lo que el estudiante ya ha aprendido o consultado.\n",
        "El reto que te dejo es que explores cómo integrar chains en tu aplicación para que puedas personalizar las respuestas del modelo según el historial de conversación. Esto te permitirá aprovechar al máximo la flexibilidad y potencia de LangChain para crear experiencias conversacionales más inteligentes y eficaces."
      ],
      "metadata": {
        "id": "xSSz9ybsVrmz"
      }
    }
  ]
}