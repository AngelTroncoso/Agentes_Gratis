{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AngelTroncoso/Agentes_Gratis/blob/main/Construccion_Agente_Conversacional.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embeddings"
      ],
      "metadata": {
        "id": "FT1FtZR-KNwJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KwWuWmjAKGbi"
      },
      "outputs": [],
      "source": [
        "!pip install langchain-openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "if \"OPENAI_API_KEY\" not in os.environ:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")"
      ],
      "metadata": {
        "id": "sqd-HYFnKP4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "embeddings_model = OpenAIEmbeddings()\n",
        "embeddings = embeddings_model.embed_documents(\n",
        "    [\n",
        "        'Hola!',\n",
        "        'Holas, cómo estás?',\n",
        "        'Cual es tu nombre?',\n",
        "        'Me llamo Daniel',\n",
        "        'Hola Daniel'\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "F8K54LvPKRWM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings"
      ],
      "metadata": {
        "id": "Mh8JzcwtKTBj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(embeddings[0])"
      ],
      "metadata": {
        "id": "s9i4ghJHKUlQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedded_query = embeddings_model.embed_query('Cual es el nombre mencionado en la conversación?')"
      ],
      "metadata": {
        "id": "dkDnO90DKWMb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedded_query"
      ],
      "metadata": {
        "id": "IpuGLkDzKXoq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "thmYPxzyKYxA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Resumen\n",
        "RAG es un enfoque de generación de respuestas donde un language model combina su conocimiento preentrenado con la capacidad de recuperar información relevante desde un vector store. En lugar de generar respuestas basadas solo en los datos con los que fue entrenado, el modelo busca y extrae fragmentos de texto relevantes almacenados externamente y genera respuestas más precisas y contextuales. Esto es especialmente útil para tareas que requieren acceso a información actualizada o específica, como documentos internos, informes empresariales, o investigaciones científicas.\n",
        "\n",
        "Componentes Clave en el Proceso de RAG\n",
        "Language Model (Modelo de Lenguaje): Es el modelo que genera las respuestas. Puede ser un modelo preentrenado como GPT-4 o una opción open-source como los modelos de Hugging Face. Este modelo actúa como la base del asistente conversacional y es quien finalmente forma las respuestas para el usuario.\n",
        "Vector Store: Es una base de datos que almacena los documentos en forma de vectores. Los documentos son fragmentados en partes más pequeñas (conocidos como chunks), y cada fragmento se convierte en un vector numérico que representa el significado semántico de ese texto. En esta clase hemos utilizado ChromaDB para gestionar este proceso.\n",
        "Contexto en las Respuestas: Además de utilizar la memoria del historial de la conversación, se recuperan fragmentos relevantes desde el vector store. Esto asegura que la respuesta no solo sea coherente con la conversación, sino que también esté basada en documentos externos que el asistente pueda consultar.\n",
        "Historial de Conversación: Este componente guarda las interacciones anteriores entre el usuario y el asistente. Esto permite que el asistente no solo recuerde las preguntas recientes, sino que también genere respuestas más contextuales. En aplicaciones reales, esto es útil para mantener la coherencia en conversaciones largas o repetidas.\n",
        "Prompts Personalizados: Al configurar prompt templates, se puede definir el rol del asistente, indicando cómo debe responder, el tono que debe usar, y el formato en que debe entregar las respuestas. Este tipo de configuración permite personalizar la experiencia de usuario.\n",
        "¿Cómo Funciona el Proceso de Recuperación y Generación?\n",
        "Input del Usuario: El usuario ingresa una pregunta o consulta. Este input es procesado y enviado al retriever, que se encarga de buscar en la base vectorial los fragmentos más relevantes.\n",
        "Búsqueda en el Vector Store: El retriever utiliza la pregunta del usuario para encontrar los fragmentos o chunks más cercanos en términos de similitud semántica. Estos fragmentos son recuperados desde el vector store, en este caso ChromaDB.\n",
        "Generación de la Respuesta: El modelo de lenguaje toma los fragmentos recuperados y los utiliza como contexto adicional para generar la respuesta. Esta combinación de información permite que las respuestas estén basadas en información más reciente o específica, extraída de documentos cargados en el vector store.\n",
        "Vector Stores y ChromaDB\n",
        "Un Vector Store como ChromaDB es una base de datos optimizada para almacenar y gestionar vectores. Estos vectores son representaciones numéricas de fragmentos de texto que han sido vectorizados utilizando modelos de embeddings, como los de OpenAI. La ventaja de usar un vector store es que permite realizar búsquedas semánticas rápidas y precisas, recuperando los fragmentos más cercanos a la consulta del usuario.\n",
        "\n",
        "Fragmentación del Texto (Chunking): Para almacenar documentos en un vector store, es necesario fragmentar el texto en partes más pequeñas (chunks). Esto asegura que los fragmentos sean manejables y permite una búsqueda más precisa cuando el usuario realiza una consulta.\n",
        "Metadata en los Fragmentos: Cada fragmento almacenado en el vector store contiene metadata adicional, como el documento de origen y la página de donde fue extraído. Esto facilita la recuperación de la información precisa.\n",
        "El Rol del Historial de Conversación\n",
        "La memoria del historial de conversación juega un papel crucial en un asistente conversacional que necesita recordar interacciones previas para dar respuestas más coherentes y útiles. En este proyecto, cada vez que el usuario hace una consulta, se guarda el contexto de la interacción, permitiendo al asistente mantener un seguimiento de la conversación.\n",
        "\n",
        "Memoria Contextualizada: Al utilizar la memoria, el asistente puede recordar el nombre del usuario, el tema que se ha discutido previamente, o cualquier otro detalle relevante. Esto mejora la experiencia del usuario, haciendo que las interacciones sean más naturales y personalizadas.\n",
        "Prompts Personalizados y su Importancia\n",
        "Los Prompt Templates permiten guiar al modelo en la forma en que debe responder. Este prompt incluye:\n",
        "\n",
        "El rol del asistente: Puede ser, por ejemplo, un experto en inteligencia artificial o un asesor de viajes.\n",
        "El formato de la respuesta: Esto puede incluir el tono de la respuesta, la inclusión de emojis, o la forma de estructurar la información.\n",
        "Instrucciones adicionales: Como la necesidad de basarse en documentos previamente cargados en el vector store o evitar inventar información.\n",
        "Aplicaciones de RAG en el Mundo Real\n",
        "El enfoque RAG es especialmente útil en escenarios donde el acceso a información actualizada o específica es esencial. Algunos casos de uso incluyen:\n",
        "\n",
        "Consultas sobre Documentación Técnica o Corporativa: Empresas que necesitan acceso rápido a manuales, informes, o políticas internas.\n",
        "Asistentes de Investigación Científica: Donde se requiere acceso a publicaciones científicas o estudios recientes.\n",
        "Sistemas de Atención al Cliente: Capaces de consultar bases de datos de conocimiento para resolver problemas técnicos en tiempo real."
      ],
      "metadata": {
        "id": "NDeN-TAHt2Zt"
      }
    }
  ]
}